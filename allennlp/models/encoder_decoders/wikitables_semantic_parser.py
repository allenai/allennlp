from typing import Dict, Generator, List, Set

from overrides import overrides

import torch
from torch.autograd import Variable
from torch.nn.modules.rnn import LSTMCell
from torch.nn.modules.linear import Linear

from allennlp.common import Params
from allennlp.common.checks import ConfigurationError
from allennlp.data import Vocabulary
from allennlp.data.dataset_readers.seq2seq import START_SYMBOL, END_SYMBOL
from allennlp.modules import Attention, TextFieldEmbedder, Seq2SeqEncoder
from allennlp.modules.similarity_functions import SimilarityFunction
from allennlp.modules.token_embedders import Embedding
from allennlp.models.model import Model
from allennlp.nn import util
from allennlp.nn.decoding import BeamSearch, DecoderTrainer, DecoderState, DecoderStep


@Model.register("wikitables_parser")
class WikiTablesSemanticParser(Model):
    """
    A ``WikiTablesSemanticParser`` is a :class:`Model` which takes as input a table and a question,
    and produces a logical form that answers the question when executed over the table.  The
    logical form is generated by a `type-constrained`, `transition-based` parser.  This is a
    re-implementation of the model used for the paper `Neural Semantic Parsing with Type
    Constraints for Semi-Structured Tables
    <https://www.semanticscholar.org/paper/Neural-Semantic-Parsing-with-Type-Constraints-for-Krishnamurthy-Dasigi/8c6f58ed0ebf379858c0bbe02c53ee51b3eb398a>`_,
    by Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner (EMNLP 2017).

    WORK STILL IN PROGRESS.  This is just copying the SimpleSeq2Seq model for now, and we'll
    iteratively improve it until we've reproduced the performance of the original parser.

    Parameters
    ----------
    vocab : ``Vocabulary``
    question_embedder : ``TextFieldEmbedder``
        Embedder for questions.
    encoder : ``Seq2SeqEncoder``
        The encoder to use for the input question.
    decoder_trainer : ``DecoderTrainer``
        The structured learning algorithm used to train the decoder (which also trains the encoder,
        but it's applied to the decoder outputs).
    decoder_beam_search : ``BeamSearch``
        When we're not training, this is how we will do decoding.
    max_decoding_steps : ``int``
        When we're decoding with a beam search, what's the maximum number of steps we should take?
        This only applies at evaluation time, not during training.
    action_namespace : ``str``
        Parser actions are represented as strings in the data.  This is the namespace in the
        vocabulary that was used to convert them into integers.
    action_embedding_dim : ``int``
        Parser actions get embeddings in this model, so we can use action histories as decoder
        inputs.  This specifies the dimensionality of action embeddings.
    attention_function: ``SimilarityFunction``
        We compute an attention over the input question at each step of the decoder, using the
        decoder hidden state as the query.  This is the similarity function we use for that
        attention.
    """
    def __init__(self,
                 vocab: Vocabulary,
                 question_embedder: TextFieldEmbedder,
                 encoder: Seq2SeqEncoder,
                 decoder_trainer: DecoderTrainer,
                 decoder_beam_search: BeamSearch,
                 max_decoding_steps: int,
                 action_namespace: str,
                 action_embedding_dim: int,
                 attention_function: SimilarityFunction) -> None:
        super(WikiTablesSemanticParser, self).__init__(vocab)
        self._source_embedder = question_embedder
        self._encoder = encoder
        self._decoder_trainer = decoder_trainer
        self._beam_search = decoder_beam_search
        self._max_decoding_steps = max_decoding_steps
        self._action_namespace = action_namespace

        num_actions = self.vocab.get_vocab_size(self._action_namespace)
        self._start_index = self.vocab.get_token_index(START_SYMBOL, self._action_namespace)
        self._end_index = self.vocab.get_token_index(END_SYMBOL, self._action_namespace)
        self._decoder_step = WikiTablesDecoderStep(self._encoder.get_output_dim(),
                                                   action_embedding_dim,
                                                   num_actions,
                                                   attention_function,
                                                   self._start_index)

    @overrides
    def forward(self,  # type: ignore
                question: Dict[str, torch.LongTensor],
                target_action_sequences: torch.LongTensor = None) -> Dict[str, torch.Tensor]:
        # pylint: disable=arguments-differ
        """
        Decoder logic for producing the entire target sequence.

        Parameters
        ----------
        question : Dict[str, torch.LongTensor]
           The output of ``TextField.as_array()`` applied on the question ``TextField``. This will
           be passed through a ``TextFieldEmbedder`` and then through an encoder.
        target_action_sequences : torch.LongTensor, optional (default = None)
           A list of possibly valid action sequences, with shape ``(batch_size, num_sequences,
           sequence_length, 1)``.  The trailing dimension is because of how our ``ListFields``
           work, and will be stripped in this method with a ``squeeze``.
        """
        # (batch_size, input_sequence_length, encoder_output_dim)
        embedded_input = self._source_embedder(question)
        source_mask = util.get_text_field_mask(question)
        batch_size = embedded_input.size(0)
        if batch_size != 1:
            raise ConfigurationError("This implementation does not currently handle batched "
                                     "inputs.  Please set the batch size of your dataset reader "
                                     "to 1.")

        encoder_outputs = self._encoder(embedded_input, source_mask)

        final_encoder_output = encoder_outputs[:, -1]  # (batch_size, encoder_output_dim)
        decoder_context = Variable(encoder_outputs.data.new(batch_size, self._encoder.get_output_dim())
                                   .fill_(0))

        if target_action_sequences is not None:
            # Remove batch dimension and trailing dimension (from ListField[LabelField]]).
            target_action_sequences = target_action_sequences.squeeze(0).squeeze(-1)
            # TODO(mattg): might be better to not hard-code the 0 here.
            target_mask = target_action_sequences > 0
        else:
            target_mask = None

        initial_score = Variable(embedded_input.data.new(1).fill_(0))
        initial_state = WikiTablesDecoderState([],
                                               initial_score,
                                               encoder_outputs=encoder_outputs,
                                               encoder_output_mask=source_mask.float(),
                                               hidden_state=(final_encoder_output, decoder_context),
                                               end_index=self._end_index)
        if self.training:
            return self._decoder_trainer.decode(initial_state,
                                                self._decoder_step,
                                                target_action_sequences,
                                                target_mask)
        else:
            outputs = {}
            if target_action_sequences is not None:
                num_steps = target_action_sequences.size(1) - 1
                outputs['loss'] = self._decoder_trainer.decode(initial_state,
                                                               self._decoder_step,
                                                               target_action_sequences,
                                                               target_mask)['loss']
            else:
                num_steps = self._max_decoding_steps
            best_final_states = self._beam_search.search(num_steps, initial_state, self._decoder_step)
            best_final_state = best_final_states[0]
            outputs['best_action_sequence'] = best_final_state.action_history
            # TODO(matt): compute accuracy here.
            return outputs

    @overrides
    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        This method overrides ``Model.decode``, which gets called after ``Model.forward``, at test
        time, to finalize predictions.  This is (confusingly) a separate notion from the "decoder"
        in "encoder/decoder", where that decoder logic lives in ``WikiTablesDecoderStep``.

        This method trims the output predictions to the first end symbol, replaces indices with
        corresponding tokens, and adds a field called ``predicted_tokens`` to the ``output_dict``.
        """
        best_action_indices = output_dict["best_action_sequence"]
        end_index = self.vocab.get_token_index(END_SYMBOL, self._action_namespace)
        action_strings = []
        for action_index in best_action_indices:
            # Collect indices till the first end_symbol
            if action_index == end_index:
                break
            action_strings.append(self.vocab.get_token_from_index(action_index,
                                                                  namespace=self._action_namespace))
        output_dict["predicted_actions"] = action_strings
        return output_dict

    @classmethod
    def from_params(cls, vocab, params: Params) -> 'WikiTablesSemanticParser':
        source_embedder_params = params.pop("question_embedder")
        question_embedder = TextFieldEmbedder.from_params(vocab, source_embedder_params)
        encoder = Seq2SeqEncoder.from_params(params.pop("encoder"))
        max_decoding_steps = params.pop("max_decoding_steps")
        action_namespace = params.pop("action_namespace")
        action_embedding_dim = params.pop("action_embedding_dim", None)
        decoder_trainer = DecoderTrainer.from_params(params.pop("decoder_trainer"))
        decoder_beam_search = BeamSearch.from_params(params.pop("decoder_beam_search"))
        # If no attention function is specified, we should not use attention, not attention with
        # default similarity function.
        attention_function_type = params.pop("attention_function", None)
        if attention_function_type is not None:
            attention_function = SimilarityFunction.from_params(attention_function_type)
        else:
            attention_function = None
        return cls(vocab,
                   question_embedder=question_embedder,
                   encoder=encoder,
                   decoder_trainer=decoder_trainer,
                   decoder_beam_search=decoder_beam_search,
                   max_decoding_steps=max_decoding_steps,
                   action_namespace=action_namespace,
                   action_embedding_dim=action_embedding_dim,
                   attention_function=attention_function)


class WikiTablesDecoderState(DecoderState):
    """
    Parameters
    ----------
    action_history : ``List[int]``
        The list of actions taken so far in this state.

        The type annotation says this is an ``int``, but none of the training logic relies on this
        being an ``int``.  In some cases, items from this list will get passed as inputs to
        ``DecodeStep``, so this must return items that are compatible with inputs to your
        ``DecodeStep`` class.
    """
    def __init__(self,
                 action_history: List[int],
                 score: torch.Tensor,
                 encoder_outputs: torch.Tensor,
                 encoder_output_mask: torch.Tensor,
                 hidden_state: torch.Tensor,
                 end_index: int) -> None:
        super(WikiTablesDecoderState, self).__init__(action_history, score)
        self.encoder_outputs = encoder_outputs
        self.encoder_output_mask = encoder_output_mask
        self.hidden_state = hidden_state
        self.end_index = end_index

    def get_output_mask(self) -> torch.Tensor:  # pylint: disable=no-self-use
        return None

    def get_valid_actions(self) -> Set[int]:  # pylint: disable=no-self-use
        return None

    def is_finished(self) -> bool:
        return len(self.action_history) > 0 and self.action_history[-1] == self.end_index


class WikiTablesDecoderStep(DecoderStep[WikiTablesDecoderState]):
    def __init__(self,
                 encoder_output_dim: int,
                 action_embedding_dim: int,
                 num_actions: int,
                 attention_function: SimilarityFunction,
                 start_index: int) -> None:
        super(WikiTablesDecoderStep, self).__init__()
        # Decoder output dim needs to be the same as the encoder output dim since we initialize the
        # hidden state of the decoder with that of the final hidden states of the encoder.
        self._output_dim = encoder_output_dim
        self._action_embedder = Embedding(num_actions, action_embedding_dim)
        self._input_attention = Attention(attention_function)
        self._start_index = start_index

        # The output of attention, which is a weighted average over encoder outputs, will be
        # concatenated to the input vector of the decoder at each time step.
        self._decoder_input_dim = encoder_output_dim + action_embedding_dim

        # TODO(pradeep): Do not hardcode decoder cell type.
        self._decoder_cell = LSTMCell(self._decoder_input_dim, self._output_dim)
        self._output_projection_layer = Linear(self._output_dim, num_actions)

    @overrides
    def take_step(self,
                  state: WikiTablesDecoderState,
                  allowed_actions: Set[int] = None) -> Generator[WikiTablesDecoderState, None, None]:
        # Outline here: first we'll construct the input to the decoder, which is a concatenation of
        # an embedding of the decoder input (the last action taken) and an attention over the
        # question.
        decoder_hidden, decoder_context = state.hidden_state
        action = state.action_history[-1] if state.action_history else self._start_index
        action_input = Variable(decoder_hidden.data.new(1).long().fill_(action))
        embedded_input = self._action_embedder(action_input)
        # (batch_size, input_sequence_length)
        input_weights = self._input_attention(decoder_hidden,
                                              state.encoder_outputs,
                                              state.encoder_output_mask)
        # (batch_size, encoder_output_dim)
        attended_input = util.weighted_sum(state.encoder_outputs, input_weights)
        # (batch_size, encoder_output_dim + target_embedding_dim)
        decoder_input = torch.cat((attended_input, embedded_input), -1)
        decoder_hidden, decoder_context = self._decoder_cell(decoder_input,
                                                             (decoder_hidden, decoder_context))

        # (batch_size, num_actions)
        logits = self._output_projection_layer(decoder_hidden)
        output_mask = state.get_output_mask()
        log_probs = util.masked_log_softmax(logits, output_mask)
        sorted_log_probs = []
        # TODO(mattg): think about how to make batching work here; batch_size is currently required
        # to be 1, so we enumerate `log_probs[0]`.
        for i, log_prob in enumerate(log_probs[0]):
            # We need to keep the gradient history, but also have a float for easy comparisons.
            # That's why we have three things here: (float_log_prob, variable_log_prob, action_id).
            sorted_log_probs.append((log_prob.data[0], log_prob, i))
        sorted_log_probs.sort(key=lambda x: -x[0])

        valid_actions = state.get_valid_actions()
        for _, log_prob, action in sorted_log_probs:
            if valid_actions is not None and action not in valid_actions:
                # If we have a constrained decoder, we need to skip any actions that are not valid
                # in the current state.
                continue
            if allowed_actions is not None and action not in allowed_actions:
                # This happens when our _decoder trainer_ wants us to only evaluate certain
                # actions, likely because they are the gold actions in this state.  We just skip
                # emitting any state that isn't allowed by the trainer, because constructing the
                # new state can be expensive.
                # TODO(mattg): is this really all that expensive?  If it isn't, it would make
                # computing metrics easier if this skipping logic happened in the trainer - the
                # trainer could keep track of when the top action wasn't allowed.
                continue
            new_action_history = state.action_history + [action]
            new_score = util.logsumexp(torch.cat([state.score, log_prob]))
            new_hidden_state = (decoder_hidden, decoder_context)
            yield WikiTablesDecoderState(new_action_history,
                                         new_score,
                                         state.encoder_outputs,
                                         state.encoder_output_mask,
                                         new_hidden_state,
                                         state.end_index)
