from collections import defaultdict
from typing import Dict, List, Set, Tuple

from overrides import overrides

import torch
from torch.autograd import Variable
from torch.nn.modules.rnn import LSTMCell
from torch.nn.modules.linear import Linear

from allennlp.common import Params
from allennlp.common import util as common_util
from allennlp.data import Vocabulary
from allennlp.data.dataset_readers.seq2seq import START_SYMBOL, END_SYMBOL
from allennlp.modules import Attention, TextFieldEmbedder, Seq2SeqEncoder
from allennlp.modules.similarity_functions import SimilarityFunction
from allennlp.modules.token_embedders import Embedding
from allennlp.models.model import Model
from allennlp.nn import util
from allennlp.nn.decoding import BeamSearch, DecoderTrainer, DecoderState, DecoderStep


@Model.register("wikitables_parser")
class WikiTablesSemanticParser(Model):
    """
    A ``WikiTablesSemanticParser`` is a :class:`Model` which takes as input a table and a question,
    and produces a logical form that answers the question when executed over the table.  The
    logical form is generated by a `type-constrained`, `transition-based` parser.  This is a
    re-implementation of the model used for the paper `Neural Semantic Parsing with Type
    Constraints for Semi-Structured Tables
    <https://www.semanticscholar.org/paper/Neural-Semantic-Parsing-with-Type-Constraints-for-Krishnamurthy-Dasigi/8c6f58ed0ebf379858c0bbe02c53ee51b3eb398a>`_,
    by Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner (EMNLP 2017).

    WORK STILL IN PROGRESS.  We'll iteratively improve it until we've reproduced the performance of
    the original parser.

    Parameters
    ----------
    vocab : ``Vocabulary``
    question_embedder : ``TextFieldEmbedder``
        Embedder for questions.
    encoder : ``Seq2SeqEncoder``
        The encoder to use for the input question.
    decoder_trainer : ``DecoderTrainer``
        The structured learning algorithm used to train the decoder (which also trains the encoder,
        but it's applied to the decoder outputs).
    decoder_beam_search : ``BeamSearch``
        When we're not training, this is how we will do decoding.
    max_decoding_steps : ``int``
        When we're decoding with a beam search, what's the maximum number of steps we should take?
        This only applies at evaluation time, not during training.
    action_namespace : ``str``
        Parser actions are represented as strings in the data.  This is the namespace in the
        vocabulary that was used to convert them into integers.
    action_embedding_dim : ``int``
        Parser actions get embeddings in this model, so we can use action histories as decoder
        inputs.  This specifies the dimensionality of action embeddings.
    attention_function: ``SimilarityFunction``
        We compute an attention over the input question at each step of the decoder, using the
        decoder hidden state as the query.  This is the similarity function we use for that
        attention.
    """
    def __init__(self,
                 vocab: Vocabulary,
                 question_embedder: TextFieldEmbedder,
                 encoder: Seq2SeqEncoder,
                 decoder_trainer: DecoderTrainer,
                 decoder_beam_search: BeamSearch,
                 max_decoding_steps: int,
                 action_namespace: str,
                 action_embedding_dim: int,
                 attention_function: SimilarityFunction) -> None:
        super(WikiTablesSemanticParser, self).__init__(vocab)
        self._question_embedder = question_embedder
        self._encoder = encoder
        self._decoder_trainer = decoder_trainer
        self._beam_search = decoder_beam_search
        self._max_decoding_steps = max_decoding_steps
        self._action_namespace = action_namespace
        self._action_padding_index = 0 if self.vocab.is_padded(self._action_namespace) else -1

        self._start_index = self.vocab.get_token_index(START_SYMBOL, self._action_namespace)
        self._end_index = self.vocab.get_token_index(END_SYMBOL, self._action_namespace)
        # This is a _global_ mapping because eventually we will have additional allowed actions for
        # each instance (e.g., instance-specific entities or predicates), and for the current
        # decoder state (e.g., you can only produce variables inside of a lambda expression).
        # These are just the actions that are specified by the grammar.
        self._global_type_productions = _get_type_productions(self.vocab, self._action_namespace)
        self._decoder_step = WikiTablesDecoderStep(vocab=vocab,
                                                   action_namespace=action_namespace,
                                                   encoder_output_dim=self._encoder.get_output_dim(),
                                                   action_embedding_dim=action_embedding_dim,
                                                   attention_function=attention_function,
                                                   start_index=self._start_index)

    @overrides
    def forward(self,  # type: ignore
                question: Dict[str, torch.LongTensor],
                table: Dict[str, torch.LongTensor],
                target_action_sequences: torch.LongTensor = None) -> Dict[str, torch.Tensor]:
        # pylint: disable=arguments-differ
        # pylint: disable=unused-argument
        """
        Decoder logic for producing the entire target sequence.

        Parameters
        ----------
        question : Dict[str, torch.LongTensor]
           The output of ``TextField.as_array()`` applied on the question ``TextField``. This will
           be passed through a ``TextFieldEmbedder`` and then through an encoder.
        table : ``Dict[str, torch.LongTensor]``
            The output of ``KnowledgeGraphField.as_array()`` applied on the table
            ``KnowledgeGraphField``.  This output is similar to a ``TextField`` output, where each
            entity in the table is treated as a "token", and we will use a ``TextFieldEmbedder`` to
            get embeddings for each entity.
        target_action_sequences : torch.LongTensor, optional (default = None)
           A list of possibly valid action sequences, with shape ``(batch_size, num_sequences,
           sequence_length, 1)``.  The trailing dimension is because of how our ``ListFields``
           work, and will be stripped in this method with a ``squeeze``.
        """
        # (batch_size, question_length, embedding_dim)
        embedded_input = self._question_embedder(question)
        question_mask = util.get_text_field_mask(question).float()
        batch_size = embedded_input.size(0)

        # (batch_size, question_length, encoder_output_dim)
        encoder_outputs = self._encoder(embedded_input, question_mask)

        # This will be our initial hidden state and memory cell for the decoder LSTM.
        final_encoder_output = encoder_outputs[:, -1]  # (batch_size, encoder_output_dim)
        memory_cell = Variable(encoder_outputs.data.new(batch_size, self._encoder.get_output_dim()).fill_(0))

        if target_action_sequences is not None:
            # Remove batch dimension and trailing dimension (from ListField[LabelField]]).
            target_action_sequences = target_action_sequences.squeeze(-1)
            target_mask = target_action_sequences != self._action_padding_index
        else:
            target_mask = None

        initial_score = Variable(embedded_input.data.new(batch_size).fill_(0))
        attended_question = self._decoder_step.attend_on_question(final_encoder_output,
                                                                  encoder_outputs,
                                                                  question_mask)

        # To make grouping states together in the decoder easier, we convert the batch dimension in
        # all of our tensors into an outer list.  For instance, the encoder outputs have shape
        # `(batch_size, question_length, encoder_output_dim)`.  We need to convert this into a list
        # of `batch_size` tensors, each of shape `(question_length, encoder_output_dim)`.  Then we
        # won't have to do any index selects, or anything, we'll just do some `torch.cat()`s.
        initial_score_list = [initial_score[i] for i in range(batch_size)]
        initial_hidden_state = [final_encoder_output[i] for i in range(batch_size)]
        initial_memory_cell = [memory_cell[i] for i in range(batch_size)]
        initial_attended_question = [attended_question[i] for i in range(batch_size)]
        encoder_output_list = [encoder_outputs[i] for i in range(batch_size)]
        question_mask_list = [question_mask[i] for i in range(batch_size)]
        initial_state = WikiTablesDecoderState(batch_indices=list(range(batch_size)),
                                               action_history=[[] for _ in range(batch_size)],
                                               score=initial_score_list,
                                               non_terminal_stack=[[START_SYMBOL] for _ in range(batch_size)],
                                               hidden_state=initial_hidden_state,
                                               memory_cell=initial_memory_cell,
                                               attended_question=initial_attended_question,
                                               encoder_outputs=encoder_output_list,
                                               encoder_output_mask=question_mask_list,
                                               global_type_productions=self._global_type_productions,
                                               end_index=self._end_index)
        if self.training:
            return self._decoder_trainer.decode(initial_state,
                                                self._decoder_step,
                                                target_action_sequences,
                                                target_mask)
        else:
            outputs = {}
            if target_action_sequences is not None:
                num_steps = target_action_sequences.size(-1) - 1
                outputs['loss'] = self._decoder_trainer.decode(initial_state,
                                                               self._decoder_step,
                                                               target_action_sequences,
                                                               target_mask)['loss']
            else:
                num_steps = self._max_decoding_steps
            best_final_states = self._beam_search.search(num_steps, initial_state, self._decoder_step)
            best_action_sequences = []
            for i in range(batch_size):
                best_action_sequences.append(best_final_states[i][0].action_history)
            outputs['best_action_sequence'] = best_action_sequences
            # TODO(matt): compute accuracy here.
            return outputs

    @overrides
    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        This method overrides ``Model.decode``, which gets called after ``Model.forward``, at test
        time, to finalize predictions.  This is (confusingly) a separate notion from the "decoder"
        in "encoder/decoder", where that decoder logic lives in ``WikiTablesDecoderStep``.

        This method trims the output predictions to the first end symbol, replaces indices with
        corresponding tokens, and adds a field called ``predicted_tokens`` to the ``output_dict``.
        """
        best_action_indices = output_dict["best_action_sequence"][0]
        end_index = self.vocab.get_token_index(END_SYMBOL, self._action_namespace)
        action_strings = []
        for action_index in best_action_indices:
            # Collect indices till the first end_symbol
            if action_index == end_index:
                break
            action_strings.append(self.vocab.get_token_from_index(action_index,
                                                                  namespace=self._action_namespace))
        output_dict["predicted_actions"] = [action_strings]
        return output_dict

    @classmethod
    def from_params(cls, vocab, params: Params) -> 'WikiTablesSemanticParser':
        question_embedder_params = params.pop("question_embedder")
        question_embedder = TextFieldEmbedder.from_params(vocab, question_embedder_params)
        encoder = Seq2SeqEncoder.from_params(params.pop("encoder"))
        max_decoding_steps = params.pop("max_decoding_steps")
        action_namespace = params.pop("action_namespace")
        action_embedding_dim = params.pop("action_embedding_dim", None)
        decoder_trainer = DecoderTrainer.from_params(params.pop("decoder_trainer"))
        decoder_beam_search = BeamSearch.from_params(params.pop("decoder_beam_search"))
        # If no attention function is specified, we should not use attention, not attention with
        # default similarity function.
        attention_function_type = params.pop("attention_function", None)
        if attention_function_type is not None:
            attention_function = SimilarityFunction.from_params(attention_function_type)
        else:
            attention_function = None
        return cls(vocab,
                   question_embedder=question_embedder,
                   encoder=encoder,
                   decoder_trainer=decoder_trainer,
                   decoder_beam_search=decoder_beam_search,
                   max_decoding_steps=max_decoding_steps,
                   action_namespace=action_namespace,
                   action_embedding_dim=action_embedding_dim,
                   attention_function=attention_function)


def _get_type_productions(vocab: Vocabulary, namespace: str) -> Dict[str, List[int]]:
    """
    Temporary method, until we get the type declaration (grammar) to build this for us.

    This method takes all of the actions that we saw in the training data and constructs a
    "grammar" from them, where here "grammar" means "valid productions for any type".  That is, if
    we see the action "r -> [<e,r>, e]", we add "[<e,r>, e]" to the valid productions for type "r"
    (in practice, we just add the action's ID in the vocabulary, which amounts to the same thing).
    This is effectively making a context-free assumption, that any production rule we ever see used
    is valid in any context.  This assumption isn't actually true, especially for terminal
    productions, but it will do for now.

    We return the list of productions for each type as a ``List[int]``, where the ``int`` is the
    `id` of the production rule in the vocabulary.  This is another thing that needs to change, as
    we will have production rules at test time that we've never seen at training time, and thus
    won't be able to predict them.  But we'll worry about that later.
    """
    type_productions: Dict[str, List[int]] = defaultdict(list)
    for action_index in range(vocab.get_vocab_size(namespace)):
        action = vocab.get_token_from_index(action_index, namespace)
        if ' -> ' in action:
            non_terminal, _ = action.split(' -> ')
            type_productions[non_terminal].append(action_index)
        else:
            # The first action we predict is the type of the logical form, not a production rule.
            # These actions don't have ' -> ' in them, and are just the type.  They are only valid
            # productions in the initial state of the decoder.
            if action != START_SYMBOL and action != END_SYMBOL:
                type_productions[START_SYMBOL].append(action_index)
    return type_productions


# This syntax is pretty weird and ugly, but it's necessary to make mypy happy with the API that
# we've defined.  We're using generics to make the types of `combine_states` and `split_finished`
# come out right.  See the note in `nn.decoding.decoder_state.py` for a little more detail.
class WikiTablesDecoderState(DecoderState['WikiTablesDecoderState']):
    """
    Parameters
    ----------
    batch_indices : ``List[int]``
        Passed to super class; see docs there.
    action_history : ``List[List[int]]``
        Passed to super class; see docs there.
    score : ``List[Variable]``
        Passed to super class; see docs there.
    non_terminal_stack : ``List[List[str]]``
        Holds the list of non-terminals that still need to be expanded for each element of the
        group.  This starts out as [START_SYMBOL], and decoding ends when this is empty.  Every
        time we take an action, we update the non-terminal stack, and we use what's on the stack to
        decide which actions are valid in the current state.
    hidden_state : ``List[Variable]``
        This holds the LSTM hidden state for each element of the group.  Each variable has shape
        ``(decoder_output_dim,)``.
    memory_cell : ``List[Variable]``
        This holds the LSTM memory cell for each element of the group.  Each variable has shape
        ``(decoder_output_dim,)``.
    attended_question : ``List[Variable]``
        This holds the attention-weighted sum over the question representations that we computed in
        the previous timestep, for each element in the group.  We keep this as part of the state
        because we use the previous attention as part of our decoder cell update.  Each variable in
        this list has shape ``(encoder_output_dim,)``.
    encoder_outputs : ``List[Variable]``
        A list of variables, each of shape ``(question_length, encoder_output_dim)``, containing
        the encoder outputs at each timestep.  The list is over batch elements, and we do the input
        this way so we can easily do a ``torch.cat`` on a list of indices into this batched list.

        Note that all of the above lists are of length ``group_size``, while the encoder outputs
        and mask are lists of length ``batch_size``.  We always pass around the encoder outputs and
        mask unmodified, regardless of what's in the grouping for this state.  We'll use the
        ``batch_indices`` for the group to pull pieces out of these lists when we're ready to
        actually do some computation.
    encoder_output_mask : ``List[Variable]``
        A list of variables, each of shape ``(question_length,)``, containing a mask over question
        tokens for each batch instance.  This is a list over batch elements, for the same reasons
        as above.
    global_type_productions : ``Dict[str, List[int]]``
        A mapping from type strings to valid action indices.  The type strings correspond to the
        non-terminals on the ``non_terminal_stack``, and the action indices (for now) correspond to
        actions in the action vocabulary.
    end_index : ``int``
        This is the index of the stop symbol, which we need so we know if we've emitted a stop
        action and are thus finished.
    """
    def __init__(self,
                 batch_indices: List[int],
                 action_history: List[List[int]],
                 score: Variable,
                 non_terminal_stack: List[List[str]],
                 hidden_state: List[Variable],
                 memory_cell: List[Variable],
                 attended_question: List[Variable],
                 encoder_outputs: Variable,
                 encoder_output_mask: Variable,
                 global_type_productions: Dict[str, List[int]],
                 end_index: int) -> None:
        super(WikiTablesDecoderState, self).__init__(batch_indices, action_history, score)
        self.non_terminal_stack = non_terminal_stack
        self.hidden_state = hidden_state
        self.memory_cell = memory_cell
        self.attended_question = attended_question
        self.encoder_outputs = encoder_outputs
        self.encoder_output_mask = encoder_output_mask
        self.global_type_productions = global_type_productions
        self.end_index = end_index

    def get_valid_actions(self) -> List[List[int]]:
        """
        Returns a list of valid actions for each element of the group.
        """
        # TODO(matt): this is going to need to look at more than just the global type productions
        # eventually - we'll need instance- and state-specific additions.  And we'll probably need
        # to return strings, instead of integers, because instance-specific production rules won't
        # be in a global vocabulary.
        return [self.global_type_productions[stack[-1]] for stack in self.non_terminal_stack]

    @staticmethod
    def update_non_terminal_stack(stack: List[str], action: str) -> List[str]:
        """
        Given a single non-terminal stack (`not` a grouped one), and an action that was taken,
        update the non-terminal stack.  This involves popping the non-terminal that was expanded
        off of the stack, then pushing on any non-terminals in the production rule back on the
        stack.  We push the non-terminals on in `reverse` order, so that the first non-terminal in
        the production rule gets popped off the stack first.

        For example, if ``stack`` is ``["r", "<e,r>", "d"]``, and ``action`` is ``d -> [<e,d>, e]``,
        the result will be ``["r", "<e,r>", "e", "<e,d>"]``.
        """
        if ' -> ' in action:
            # TODO(mattg,pradeep): we need to handle lambdas specially here, to update the valid
            # productions for the type of the variable.
            non_terminal, production_string = action.split(' -> ')
            assert stack[-1] == non_terminal
            new_stack = stack[:-1]
            productions = WikiTablesDecoderState.get_productions_from_string(production_string)
            for production in reversed(productions):
                if WikiTablesDecoderState.is_non_terminal(production):
                    new_stack.append(production)
            return new_stack
        else:
            # The first action we take is just predicting a type, so doesn't have ' -> ' in it.  In
            # this case, we had better be trying to expand the START_SYMBOL, and our next
            # non-terminal stack is just the type that we predicted.
            assert stack == [START_SYMBOL]
            return [action]

    @staticmethod
    def get_productions_from_string(production_string: str) -> List[str]:
        """
        Takes a string like '[<d,d>, d]' and parses it into a list like ['<d,d>', 'd'].  For
        production strings that are not lists, like '<e,d>', we return a single-element list:
        ['<e,d>'].
        """
        if production_string[0] == '[':
            return production_string[1:-1].split(', ')
        else:
            return [production_string]

    @staticmethod
    def is_non_terminal(production: str) -> bool:
        # TODO(mattg): these static methods probably belong in some other class.
        if production[0] == '<':
            return True
        if production.startswith('cell'):
            return False
        return production[0].islower()

    # @overrides  - overrides can't handle the generics we're using here, apparently
    def is_finished(self) -> bool:
        if len(self.batch_indices) != 1:
            raise RuntimeError("is_finished() is only defined with a group_size of 1")
        return not self.non_terminal_stack[0]

    # @overrides  - overrides can't handle the generics we're using here, apparently
    def split_finished(self) -> Tuple['WikiTablesDecoderState', 'WikiTablesDecoderState']:
        # We keep track of both of these so we can efficiently decide whether we need to split at
        # all.
        finished_indices = []
        not_finished_indices = []
        for i, stack in enumerate(self.non_terminal_stack):
            if stack:
                not_finished_indices.append(i)
            else:
                finished_indices.append(i)

        # Return value is (finished, not_finished)
        if not finished_indices:
            return (None, self)
        if not not_finished_indices:
            return (self, None)
        finished_state = self._make_new_state_with_group_indices(finished_indices)
        not_finished_state = self._make_new_state_with_group_indices(not_finished_indices)
        return (finished_state, not_finished_state)

    @classmethod
    # @overrides  - overrides can't handle the generics we're using here, apparently
    def combine_states(cls, states: List['WikiTablesDecoderState']) -> 'WikiTablesDecoderState':
        batch_indices = [batch_index for state in states for batch_index in state.batch_indices]
        action_histories = [action_history for state in states for action_history in state.action_history]
        scores = [score for state in states for score in state.score]
        non_terminal_stacks = [stack for state in states for stack in state.non_terminal_stack]
        hidden_states = [hidden_state for state in states for hidden_state in state.hidden_state]
        memory_cells = [memory_cell for state in states for memory_cell in state.memory_cell]
        attended_question = [attended for state in states for attended in state.attended_question]
        return WikiTablesDecoderState(batch_indices=batch_indices,
                                      action_history=action_histories,
                                      score=scores,
                                      non_terminal_stack=non_terminal_stacks,
                                      hidden_state=hidden_states,
                                      memory_cell=memory_cells,
                                      attended_question=attended_question,
                                      encoder_outputs=states[0].encoder_outputs,
                                      encoder_output_mask=states[0].encoder_output_mask,
                                      global_type_productions=states[0].global_type_productions,
                                      end_index=states[0].end_index)

    def _make_new_state_with_group_indices(self, group_indices: List[int]) -> 'WikiTablesDecoderState':
        """
        The ``WikiTablesDecoderState`` is `grouped`.  This is batching together the computation of
        many individual states, but we're using a different word here because it's not the same
        batching as the input training examples.  This method returns a new state that contains
        only selected elements of the group.  You might use this to split the group elements in a
        state into a finished state and a not finished state, for instance, if you know which group
        elements are finished.
        """
        group_batch_indices = [self.batch_indices[i] for i in group_indices]
        group_action_histories = [self.action_history[i] for i in group_indices]
        group_scores = [self.score[i] for i in group_indices]
        group_non_terminal_stacks = [self.non_terminal_stack[i] for i in group_indices]
        group_hidden_states = [self.hidden_state[i] for i in group_indices]
        group_memory_cells = [self.memory_cell[i] for i in group_indices]
        group_attended_question = [self.attended_question[i] for i in group_indices]
        return WikiTablesDecoderState(batch_indices=group_batch_indices,
                                      action_history=group_action_histories,
                                      score=group_scores,
                                      non_terminal_stack=group_non_terminal_stacks,
                                      hidden_state=group_hidden_states,
                                      memory_cell=group_memory_cells,
                                      attended_question=group_attended_question,
                                      encoder_outputs=self.encoder_outputs,
                                      encoder_output_mask=self.encoder_output_mask,
                                      global_type_productions=self.global_type_productions,
                                      end_index=self.end_index)


class WikiTablesDecoderStep(DecoderStep[WikiTablesDecoderState]):
    def __init__(self,
                 vocab: Vocabulary,
                 action_namespace: str,
                 encoder_output_dim: int,
                 action_embedding_dim: int,
                 attention_function: SimilarityFunction,
                 start_index: int) -> None:
        super(WikiTablesDecoderStep, self).__init__()
        self._vocab = vocab
        self._action_namespace = action_namespace
        num_actions = vocab.get_vocab_size(action_namespace)
        self._action_embedder = Embedding(num_actions, action_embedding_dim)
        self._input_attention = Attention(attention_function)
        self._start_index = start_index

        # Decoder output dim needs to be the same as the encoder output dim since we initialize the
        # hidden state of the decoder with the final hidden state of the encoder.
        output_dim = encoder_output_dim
        input_dim = output_dim
        # Our decoder input will be the concatenation of the decoder hidden state and the previous
        # action embedding, and we'll project that down to the decoder's `input_dim`, which we
        # arbitrarily set to be the same as `output_dim`.
        self._input_projection_layer = Linear(output_dim + action_embedding_dim, input_dim)
        # Before making a prediction, we'll compute an attention over the input given our updated
        # hidden state.  Then we concatenate that with the decoder state and project to
        # `action_embedding_dim` to make a prediction.
        self._output_projection_layer = Linear(output_dim + encoder_output_dim, action_embedding_dim)

        # TODO(pradeep): Do not hardcode decoder cell type.
        self._decoder_cell = LSTMCell(input_dim, output_dim)

    @overrides
    def take_step(self,
                  state: WikiTablesDecoderState,
                  max_actions: int = None,
                  allowed_actions: List[Set[int]] = None) -> List[WikiTablesDecoderState]:
        # Outline here: first we'll construct the input to the decoder, which is a concatenation of
        # an embedding of the decoder input (the last action taken) and an attention over the
        # question.  Then we'll update our decoder's hidden state given this input, and recompute
        # an attention over the question given our new hidden state.  We'll use a concatenation of
        # the new hidden state and the new attention to predict an output, then yield new states.
        # Each new state corresponds to one valid action that can be taken from the current state,
        # and they are ordered by their probability of being selected.
        attended_question = torch.stack([x for x in state.attended_question])
        hidden_state = torch.stack([x for x in state.hidden_state])
        memory_cell = torch.stack([x for x in state.memory_cell])
        actions = [action_history[-1] if action_history else self._start_index
                   for action_history in state.action_history]
        action_input = Variable(hidden_state.data.new(actions).long())
        # TODO(mattg): probably makes sense to just put this in the state from the previous
        # iteration, because we're embedding actions to predict things now.
        embedded_input = self._action_embedder(action_input)

        # (group_size, decoder_input_dim)
        decoder_input = self._input_projection_layer(torch.cat([attended_question, embedded_input], -1))

        hidden_state, memory_cell = self._decoder_cell(decoder_input, (hidden_state, memory_cell))

        # (group_size, encoder_output_dim)
        encoder_outputs = torch.stack([state.encoder_outputs[i] for i in state.batch_indices])
        encoder_output_mask = torch.stack([state.encoder_output_mask[i] for i in state.batch_indices])
        attended_question = self.attend_on_question(hidden_state,
                                                    encoder_outputs,
                                                    encoder_output_mask)

        # To predict an action, we'll use a concatenation of the hidden state and attention over
        # the question.  We'll just predict an _embedding_, which we will compare to embedded
        # representations of all valid actions to get a final output.
        action_query = torch.cat([hidden_state, attended_question], dim=-1)

        # (group_size, action_embedding_dim)
        predicted_action_embedding = self._output_projection_layer(action_query)

        # (group_size, num_actions), giving the index of each valid action for each item in the
        # group.  This is actually just a list of lists, on the CPU, not a CUDA variable.
        valid_actions = state.get_valid_actions()

        # action_embeddings: (group_size, num_actions, action_embedding_dim)
        # action_mask: (group_size, num_actions)
        action_embeddings, action_mask = self._embed_actions(valid_actions)

        # We'll do a batch dot product here with `bmm`.  We want `dot(predicted_action_embedding,
        # action_embedding)` for each `action_embedding`, and we can get that efficiently with
        # `bmm` and some squeezing.
        # Shape: (batch_size, num_actions)
        action_logits = action_embeddings.bmm(predicted_action_embedding.unsqueeze(-1)).squeeze(-1)
        log_probs = util.masked_log_softmax(action_logits, action_mask.float())
        return self._compute_new_states(state,
                                        log_probs,
                                        hidden_state,
                                        memory_cell,
                                        attended_question,
                                        valid_actions,
                                        allowed_actions,
                                        max_actions)

    def attend_on_question(self,
                           query: Variable,
                           encoder_outputs: Variable,
                           encoder_output_mask: Variable) -> Variable:
        """
        Given a query (which is typically the decoder hidden state), compute an attention over the
        output of the question encoder, and return a weighted sum of the question representations
        given this attention.

        This is a simple computation, but we have it as a separate method so that the ``forward``
        method on the main parser module can call it on the initial hidden state, to simplify the
        logic in ``take_step``.
        """
        # (group_size, question_length)
        question_attention_weights = self._input_attention(query,
                                                           encoder_outputs,
                                                           encoder_output_mask)
        # (group_size, encoder_output_dim)
        return util.weighted_sum(encoder_outputs, question_attention_weights)

    def _embed_actions(self, actions: List[List[int]]) -> Tuple[Variable, Variable]:
        """
        Returns an embedded representation of the given actions.  The input is a list of valid
        actions for each element in the group, which may have variable length.  We pad these to
        have shape ``(group_size, num_actions)``, and then embed them.  Because of the padding, we
        additionally return a mask.

        Parameters
        ----------
        actions : ``List[List[int]]``
            The set of actions to embed.  This is typically a list of valid actions for each item
            in the group of the current state.

        Returns
        -------
        action_embeddings : float ``Variable``
            An embedded representation of all of the given actions.  Shape is ``(group_size,
            num_actions, action_embedding_dim)``, where ``num_actions`` is the maximum length of
            the input ``actions`` lists.
        action_mask : long ``Variable``
            A mask of shape ``(group_size, num_actions)`` indicating which ``(group_index,
            action_index)`` pairs were merely added as padding.
        """
        num_actions = [len(action_list) for action_list in actions]
        max_num_actions = max(num_actions)
        padded_actions = [common_util.pad_sequence_to_length(action_list, max_num_actions)
                          for action_list in actions]
        action_tensor = Variable(self._action_embedder.weight.data.new(padded_actions).long())
        action_embeddings = self._action_embedder(action_tensor)
        sequence_lengths = Variable(action_embeddings.data.new(num_actions))
        action_mask = util.get_mask_from_sequence_lengths(sequence_lengths, max_num_actions)
        return action_embeddings, action_mask

    def _compute_new_states(self,  # pylint: disable=no-self-use
                            state: WikiTablesDecoderState,
                            log_probs: Variable,
                            hidden_state: Variable,
                            memory_cell: Variable,
                            attended_question: Variable,
                            considered_actions: List[List[int]],
                            allowed_actions: List[Set[int]],
                            max_actions: int = None) -> List[WikiTablesDecoderState]:
        sorted_log_probs, sorted_actions = log_probs.sort(dim=-1, descending=True)
        sorted_actions = sorted_actions.data.cpu().numpy().tolist()
        best_next_states: Dict[int, List[Tuple[int, int, int]]] = defaultdict(list)
        for group_index, (batch_index, group_actions) in enumerate(zip(state.batch_indices, sorted_actions)):
            for action_index, action in enumerate(group_actions):
                # `action` is currently the index in `log_probs`, not the actual action ID.  To get
                # the action ID, we need to go through `considered_actions`.
                if action_index >= len(considered_actions[group_index]):
                    # This was padding.  We can check either `action_index` or `action` here - it's
                    # really `action` that we care about, but our masking should have sorted all of
                    # the higher actions to the end, anyway.
                    continue
                action = considered_actions[group_index][action]
                if allowed_actions is not None and action not in allowed_actions[group_index]:
                    # This happens when our _decoder trainer_ wants us to only evaluate certain
                    # actions, likely because they are the gold actions in this state.  We just skip
                    # emitting any state that isn't allowed by the trainer, because constructing the
                    # new state can be expensive.
                    continue
                best_next_states[batch_index].append((group_index, action_index, action))
        new_states = []
        for batch_index, best_states in best_next_states.items():
            if max_actions is not None:
                best_states = best_states[:max_actions]
            for group_index, action_index, action in best_states:
                # We'll yield a bunch of states here that all have a `group_size` of 1, so that the
                # learning algorithm can decide how many of these it wants to keep, and it can just
                # regroup them later, as that's a really easy operation.
                new_action_history = state.action_history[group_index] + [action]
                new_score = state.score[group_index] + sorted_log_probs[group_index, action_index]
                action_string = self._vocab.get_token_from_index(action, self._action_namespace)
                new_non_terminal_stack = state.update_non_terminal_stack(
                        state.non_terminal_stack[group_index], action_string)
                new_state = WikiTablesDecoderState(batch_indices=[state.batch_indices[group_index]],
                                                   action_history=[new_action_history],
                                                   score=[new_score],
                                                   non_terminal_stack=[new_non_terminal_stack],
                                                   hidden_state=[hidden_state[group_index]],
                                                   memory_cell=[memory_cell[group_index]],
                                                   attended_question=[attended_question[group_index]],
                                                   encoder_outputs=state.encoder_outputs,
                                                   encoder_output_mask=state.encoder_output_mask,
                                                   global_type_productions=state.global_type_productions,
                                                   end_index=state.end_index)
                new_states.append(new_state)
        return new_states
