from typing import Iterator, Dict, List, Iterable
import itertools
import logging

from overrides import overrides

from allennlp.common import Params
from allennlp.data.iterators.data_iterator import DataIterator
from allennlp.data.dataset import LazyDataset, Dataset
from allennlp.data.instance import Instance

logger = logging.getLogger(__name__)  # pylint: disable=invalid-name

@DataIterator.register("lazy-basic")
class LazyBasicIterator(DataIterator[LazyDataset]):
    """
    A very basic lazy iterator, which takes a dataset and yields fixed size batches.
    Each batch is padded to the maximum length within that batch.

    Parameters
    ----------
    batch_size : int, optional, (default = 32)
        The size of each batch of instances yielded when calling the iterator.
    instances_per_epoch: int, optional, (default = None)
        If this parameter is specified, each epoch generated by ``LazyIterator.__call__``
        contains at most this many instances. A "cursor" is stored for each Dataset,
        so that each epoch picks up where the previous one left off.

        If the value is ``None``, then each epoch is just a full pass through the Dataset.
    """
    def __init__(self, batch_size: int = 32, instances_per_epoch: int = None) -> None:
        self._batch_size = batch_size
        self._instances_per_epoch = instances_per_epoch

        # As you might use this iterator with multiple datasets,
        # you need to store a cursor for each one.
        self._cursors: Dict[LazyDataset, Iterator[Instance]] = {}

    @overrides
    def get_num_batches(self, _: LazyDataset) -> int:
        # pylint: disable=no-self-use
        # TODO(joelgrus): Figure out the right way to handle this when `instances_per_epoch`
        #   is not provided. This is definitely not right, but currently this is only used for tqdm,
        #   and the only consequence is that the tqdm for iterating over an epoch
        #   will show as a count rather than as a progress bar.
        return self._instances_per_epoch // self._batch_size if self._instances_per_epoch else 1

    def _take_instances(self, dataset: LazyDataset, max_instances: int) -> Iterator[Instance]:
        """
        Take the next `max_instances` instances from the given dataset.
        If you get to the end of the dataset, start again from the beginning.
        """
        # If we don't have a cursor for this dataset, create one.
        iterator = self._cursors.get(dataset, iter(dataset))

        while max_instances > 0:
            try:
                # If there are instances left on this iterator,
                # yield one and decrement max_instances.
                yield next(iterator)
                max_instances -= 1
            except StopIteration:
                # None left, so start over again at the beginning of the dataset.
                iterator = iter(dataset)

        # We may have a new iterator, so update the cursor.
        self._cursors[dataset] = iterator

    @overrides
    def _create_batches(self, dataset: LazyDataset, shuffle: bool) -> Iterable[Dataset]:
        if shuffle:
            # TODO(joelgrus): figure out how to configure this and then raise ConfigurationError
            logger.warning("cannot shuffle a lazy dataset")

        # Get an iterator for the next epoch
        if self._instances_per_epoch is None:
            iterator = iter(dataset)
        else:
            iterator = self._take_instances(dataset, self._instances_per_epoch)

        # Create batches. This usage of `iter` calls the provided function repeatedly,
        # yielding the resulting values until it reaches the provided sentinel.
        # Here that produces `_batch_size` lists of instances, stopping when an empty
        # list is produced (meaning that `iterator` is exhausted).
        batches: Iterator[List[Instance]] = iter(
                lambda: list(itertools.islice(iterator, 0, self._batch_size)),
                [])

        return (Dataset(batch) for batch in batches)

    @classmethod
    def from_params(cls, params: Params) -> 'LazyBasicIterator':
        batch_size = params.pop_int('batch_size', 32)
        instances_per_epoch = params.pop_int('instances_per_epoch', None)
        params.assert_empty(cls.__name__)
        return cls(batch_size=batch_size, instances_per_epoch=instances_per_epoch)
