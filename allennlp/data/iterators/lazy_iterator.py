from typing import List, Iterator, Dict
import itertools
import logging

from overrides import overrides

from allennlp.common import Params
from allennlp.data.iterators.data_iterator import DataIterator
from allennlp.data.dataset import LazyDataset
from allennlp.data.instance import Instance

logger = logging.getLogger(__name__)  # pylint: disable=invalid-name

@DataIterator.register("lazy")
class LazyIterator(DataIterator[LazyDataset]):
    """
    A very basic lazy iterator, which takes a dataset and yields fixed size batches.
    Each batch is padded to the maximum length within that batch.

    Parameters
    ----------
    batch_size : int, optional, (default = 32)
        The size of each batch of instances yielded when calling the iterator.
    instances_per_epoch: int, optional, (default = None)
        If this parameter is specified, each epoch generated by ``LazyIterator.__call__``
        contains at most this many instances. A "cursor" is stored for each Dataset,
        so that each epoch picks up where the previous one left off.

        If the value is ``None``, then each epoch is just a full pass through the Dataset.
    """
    def __init__(self, batch_size: int = 32, instances_per_epoch: int = None) -> None:
        self._batch_size = batch_size
        self._instances_per_epoch = instances_per_epoch

        # As you might use this iterator with multiple datasets,
        # you need to store a cursor for each one.
        self._cursors: Dict[LazyDataset, Iterator[Instance]] = {}

    @overrides
    def get_num_batches(self, _: LazyDataset) -> int:
        # pylint: disable=no-self-use
        # TODO(joelgrus): Figure out the right way to handle this.
        #   This is definitely not right, but currently this is only used for tqdm,
        #   and the only consequence is that the tqdm for iterating over an epoch
        #   will show as a count rather than as a progress bar.
        return 1

    def _take_instances(self, dataset: LazyDataset, max_instances: int) -> Iterator[Instance]:
        """
        Take the next at-most `max_instances` from the given dataset.
        If you get to the end of the dataset, return fewer.
        """
        # If we don't have a cursor for this dataset, create one.
        iterator = self._cursors.get(dataset, iter(dataset))

        # Check if the iterator is exhausted:
        try:
            instance = next(iterator)
            # Not exhausted, so yield the instance and decrement max_instances.
            yield instance
            max_instances -= 1
        except StopIteration:
            # Exhausted, so get a new iterator.
            iterator = iter(dataset)

        # We may have a new iterator, so update the cursor.
        self._cursors[dataset] = iterator

        # Yield at most `max_instances` instances from the iterator.
        yield from itertools.islice(iterator, max_instances)

    @overrides
    def _create_batches(self, dataset: LazyDataset, shuffle: bool) -> Iterator[List[Instance]]:
        if shuffle:
            # TODO(joelgrus): figure out how to configure this and then raise ConfigurationError
            logger.warning("cannot shuffle a lazy dataset")

        # Get an iterator for the next epoch
        if self._instances_per_epoch is None:
            iterator = iter(dataset)
        else:
            iterator = self._take_instances(dataset, self._instances_per_epoch)

        # And return it in batches. See, e.g. https://stackoverflow.com/a/31170795
        return iter(lambda: list(itertools.islice(iterator, 0, self._batch_size)), [])

    @classmethod
    def from_params(cls, params: Params) -> 'LazyIterator':
        batch_size = params.pop_int('batch_size', 32)
        instances_per_epoch = params.pop_int('instances_per_epoch', None)
        params.assert_empty(cls.__name__)
        return cls(batch_size=batch_size, instances_per_epoch=instances_per_epoch)
